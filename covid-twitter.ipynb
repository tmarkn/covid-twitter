{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-TWITTER\n",
    "\n",
    "Here we'll import all our needed modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import dateutil.parser\n",
    "import operator\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll do an import of the twitter data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath='data/twitter.json'\n",
    "localTweetList = []\n",
    "globalTweetCounter = 0\n",
    "frequencyMap = {}\n",
    "people = {}\n",
    "timeFormat = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "with open(filePath, 'r') as f:\n",
    "    tweets = json.loads(f.readline())\n",
    "    for tweet in tweets:\n",
    "        # Try to extract the time of the tweet\n",
    "        currentTime = dateutil.parser.parse(tweet['created_at'])\n",
    "        currentTime = currentTime.replace(hour=0, minute=0, second=0)\n",
    "\n",
    "        # print(currentTime)\n",
    "        # Increment tweet count\n",
    "        globalTweetCounter += 1\n",
    "        \n",
    "        # If our frequency map already has this time, use it, otherwise add\n",
    "        if currentTime in frequencyMap.keys():\n",
    "            timeMap = frequencyMap[currentTime]\n",
    "            timeMap[\"count\"] += 1\n",
    "            timeMap[\"list\"].append(tweet)\n",
    "        else:\n",
    "            frequencyMap[currentTime] = {\"count\":1, \"list\":[tweet]}\n",
    "\n",
    "        # If our user is already added, use, otherwise add\n",
    "        if tweet['user']['screen_name'] in people:\n",
    "            people[tweet['user']['screen_name']].append(tweet)\n",
    "        else:\n",
    "            people[tweet['user']['screen_name']] = [tweet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then filter take the words before and after the first case of covid-19 in the united states (2020-01-21)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc = pytz.UTC\n",
    "targetTime = utc.localize(dateutil.parser.parse('2020-01-21'))\n",
    "wordsBefore = {}\n",
    "wordsAfter = {}\n",
    "for person in people:\n",
    "    print(person, len(people[person]))\n",
    "    # count number of words used\n",
    "    wordsBefore[person] = {}\n",
    "    wordsAfter[person] = {}\n",
    "    for tweet in people[person]:\n",
    "        # split text and clean words\n",
    "        tempWords = tweet['text'].split()\n",
    "        tempWords = [deEmojify(i).strip().lower() for i in tempWords]\n",
    "        # check if before or after first case of covid and add to dictionary\n",
    "        timeCreated = dateutil.parser.parse(tweet['created_at'])\n",
    "        timeCreated = timeCreated.replace(hour=0, minute=0, second=0)\n",
    "\n",
    "        # before first case\n",
    "        if timeCreated < targetTime:\n",
    "            for word in tempWords:\n",
    "                if not word:\n",
    "                    continue\n",
    "                if word in wordsBefore[person]:\n",
    "                    wordsBefore[person][word] += 1\n",
    "                else:\n",
    "                    wordsBefore[person][word] = 1\n",
    "        # after first case \n",
    "        else:\n",
    "            for word in tempWords:\n",
    "                if not word:\n",
    "                    continue\n",
    "                if word in wordsAfter[person]:\n",
    "                    wordsAfter[person][word] += 1\n",
    "                else:\n",
    "                    wordsAfter[person][word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll remove the filler words.\n",
    "\n",
    "These words are useful in language as it makes the sentences more coherent, but we don't care about that in our dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entriesToRemove = ('the', 'and', 'to', 'of', 'a', 'an', 'are', 'in', 'is', 'on')\n",
    "adjustedBefore = wordsBefore\n",
    "adjustedAfter = wordsAfter\n",
    "for person in people:\n",
    "    for k in entriesToRemove:\n",
    "        adjustedBefore[person].pop(k, None)\n",
    "        adjustedAfter[person].pop(k, None)\n",
    "\n",
    "    adjustedBefore[person] = sorted(adjustedBefore[person].items(), key=lambda item: item[1], reverse=True)\n",
    "    adjustedAfter[person] = sorted(adjustedAfter[person].items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    print('\\t', adjustedBefore[person][:10])\n",
    "    print('\\t', adjustedAfter[person][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll plot each one"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for person in people:\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(18,6)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(bottom=0.15)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    plt.title(\"Word Frequency: \" + person + \" (Before First Case of Covid: 2020-01-21)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\") \n",
    "    plt.bar([x[0] for x in adjustedBefore[person]][:30], [x[1] for x in adjustedBefore[person]][:30], color='blue', label='Word Frequency')\n",
    "    \n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Word Frequency: \" + person + \" (After First Case of Covid: 2020-01-21)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\") \n",
    "    plt.bar([x[0] for x in adjustedAfter[person]][:30], [x[1] for x in adjustedAfter[person]][:30], color='red', label='Word Frequency')\n",
    "    ax.legend()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we'll plot frequency of tweets. We'll use this to compare to the number of cases of covid-19 later"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in any gaps\n",
    "times = sorted(frequencyMap.keys())\n",
    "firstTime = times[0]\n",
    "lastTime = times[-1]\n",
    "thisTime = firstTime\n",
    "\n",
    "#timeIntervalStep = datetime.timedelta(0, 60)    # Time step in seconds\n",
    "timeIntervalStep = datetime.timedelta(hours=24)\n",
    "while ( thisTime <= lastTime ):\n",
    "    if ( thisTime not in frequencyMap.keys() ):\n",
    "        frequencyMap[thisTime] = {\"count\":0, \"list\":[]}\n",
    "        \n",
    "    thisTime = thisTime + timeIntervalStep\n",
    "\n",
    "print (\"Processed Tweet Count:\", globalTweetCounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot it"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10,4)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.3)\n",
    "\n",
    "plt.title(\"Tweet Frequency (First case of COVID-19 at 2020-01-21)\")\n",
    "\n",
    "# Sort the times into an array for future use\n",
    "sortedTimes = sorted(frequencyMap.keys())\n",
    "\n",
    "# What time span do these tweets cover?\n",
    "print (\"Time Frame:\", sortedTimes[0], sortedTimes[-1])\n",
    "\n",
    "# Get a count of tweets per minute\n",
    "postFreqList = [frequencyMap[x][\"count\"] for x in sortedTimes]\n",
    "\n",
    "# We'll have ticks every 10 days\n",
    "smallerXTicks = range(0, len(sortedTimes), 10)\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x].strftime('%Y:%m:%d') for x in smallerXTicks], rotation=45, ha=\"right\")\n",
    "\n",
    "# Plot the post frequency\n",
    "ax.plot(range(len(frequencyMap)), [x if x > 0 else 0 for x in postFreqList], color=\"blue\", label=\"Posts\")\n",
    "ax.grid(b=True, which=u'major')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now import the covid-19 case data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covid data\n",
    "covid = np.loadtxt(\"data/us-counties.csv\", delimiter=\",\", dtype='str')\n",
    "covid = np.delete(covid, (0), axis=0)\n",
    "cases = {}\n",
    "for time in sortedTimes:\n",
    "    cases[time] = 0\n",
    "\n",
    "count = 0\n",
    "for reported in covid:\n",
    "    time = utc.localize(dateutil.parser.parse(reported[0]))\n",
    "    count += int(reported[4])\n",
    "    cases[time] = count\n",
    "\n",
    "postCaseList = np.array([cases[x] for x in cases])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and again we plot it"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10,4)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(left=0.1, bottom=0.3)\n",
    "\n",
    "plt.title(\"Reported Cases of COVID-19 (First case at 2020-01-21)\")\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x].strftime('%Y:%m:%d') for x in smallerXTicks], rotation=45, ha=\"right\")\n",
    "ax.plot(range(len(sorted([k for k in cases]))), postCaseList, color=\"red\", label='Reported Cases')\n",
    "ax.grid(b=True, which=u'major')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at hashtags"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A map for hashtag counts\n",
    "hashtagCounter = {}\n",
    "\n",
    "# For each minute, pull the list of hashtags and add to the counter\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        hashtagList = tweet[\"entities\"][\"hashtags\"]\n",
    "        \n",
    "        for hashtagObj in hashtagList:\n",
    "            \n",
    "            # We lowercase the hashtag to avoid duplicates (e.g., #MikeBrown vs. #mikebrown)\n",
    "            hashtagString = hashtagObj[\"text\"].lower()\n",
    "            \n",
    "            if ( hashtagString not in hashtagCounter ):\n",
    "                hashtagCounter[hashtagString] = 1\n",
    "            else:\n",
    "                hashtagCounter[hashtagString] += 1\n",
    "\n",
    "print (\"Unique Hashtags:\", len(hashtagCounter.keys()))\n",
    "sortedHashtags = sorted(hashtagCounter, key=hashtagCounter.get, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then plot the first 20"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10,4)\n",
    "fig.subplots_adjust(bottom=0.3)\n",
    "\n",
    "plt.title(\"Hashtag Count\")\n",
    "plt.xticks(range(20), [ht for ht in sortedHashtags][:20], rotation=45, ha=\"right\")\n",
    "ax.bar(range(20), [hashtagCounter[ht] for ht in sortedHashtags][:20], color=\"red\", label='Hashtags')\n",
    "ax.grid(b=True, which=u'major')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make a bag of words as the dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "labelsBefore = []\n",
    "labelsAfter = []\n",
    "wordsBefore = []\n",
    "wordsAfter = []\n",
    "for person in people:\n",
    "    for tweet in people[person]:\n",
    "        timeCreated = dateutil.parser.parse(tweet['created_at']).replace(hour=0, minute=0, second=0)\n",
    "        if timeCreated < targetTime:\n",
    "            wordsBefore.append(tweet['text'])\n",
    "            labelsBefore.append(person.lower())\n",
    "        else:\n",
    "            wordsAfter.append(tweet['text'])\n",
    "            labelsAfter.append(person.lower())\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizerBefore = CountVectorizer(min_df=5)\n",
    "featuresBefore = vectorizerBefore.fit_transform(wordsBefore)\n",
    "\n",
    "vectorizerAfter = CountVectorizer(min_df=5)\n",
    "featuresAfter = vectorizerAfter.fit_transform(wordsAfter)\n",
    "\n",
    "print(featuresBefore.toarray())\n",
    "print(featuresAfter.toarray())\n",
    "# print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then train the data!\n",
    "\n",
    "First we import everything we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll train on the data before the first case of covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(featuresBefore, labelsBefore, test_size=0.33, random_state=42) \n",
    "print(\"-----BEFORE-----\")\n",
    "# decision tree\n",
    "dTree = DecisionTreeClassifier()\n",
    "start_time = time.time()\n",
    "dTree = dTree.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('Decision Tree', time.time() - start_time))\n",
    "\n",
    "accuracy = dTree.score(X_test, y_test)\n",
    "print(\"decision tree accuracy: %.3f\" %accuracy)\n",
    "\n",
    "# knn\n",
    "k = 20\n",
    "predictor = neighbors.KNeighborsClassifier(k)\n",
    "start_time = time.time()\n",
    "predictor.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('KNN', time.time() - start_time))\n",
    "\n",
    "accuracy = predictor.score(X_test, y_test)\n",
    "print(\"KNN accuracy: %.3f\" %accuracy)\n",
    "\n",
    "# random forest\n",
    "predictor = RandomForestClassifier()\n",
    "start_time = time.time()\n",
    "predictor.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('random forest', time.time() - start_time))\n",
    "\n",
    "predictor.predict(X_test)\n",
    "accuracy = predictor.score(X_test, y_test)\n",
    "print(\"random forest: %.3f\" %accuracy)\n",
    "\n",
    "# logistic regression\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('LogisticRegression', time.time() - start_time))\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Log Regression: %.3f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after first case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(featuresAfter, labelsAfter, test_size=0.33, random_state=42) \n",
    "print(\"-----After-----\")\n",
    "# decision tree\n",
    "dTree = DecisionTreeClassifier()\n",
    "start_time = time.time()\n",
    "dTree = dTree.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('Decision Tree', time.time() - start_time))\n",
    "\n",
    "accuracy = dTree.score(X_test, y_test)\n",
    "print(\"decision tree accuracy: %.3f\" %accuracy)\n",
    "\n",
    "# knn\n",
    "k = 20\n",
    "predictor = neighbors.KNeighborsClassifier(k)\n",
    "start_time = time.time()\n",
    "predictor.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('KNN', time.time() - start_time))\n",
    "\n",
    "accuracy = predictor.score(X_test, y_test)\n",
    "print(\"KNN accuracy: %.3f\" %accuracy)\n",
    "\n",
    "# random forest\n",
    "predictor = RandomForestClassifier()\n",
    "start_time = time.time()\n",
    "predictor.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('random forest', time.time() - start_time))\n",
    "\n",
    "predictor.predict(X_test)\n",
    "accuracy = predictor.score(X_test, y_test)\n",
    "print(\"random forest: %.3f\" %accuracy)\n",
    "\n",
    "# logistic regression\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('LogisticRegression', time.time() - start_time))\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Log Regression: %.3f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get data not in the dataset and test in on that rather than splitting (This dataset was also retrieved using twitterData.py, but with changing the dates).\n",
    "\n",
    "This one is for 2019-08-17 to 2019-08-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleBeforeText = []\n",
    "peopleBeforeNames = []\n",
    "with open(\"data/twitterBefore.json\", 'r') as f:\n",
    "    tweets = json.loads(f.readline())\n",
    "    for tweet in tweets:\n",
    "        peopleBeforeText.append(tweet['text'])\n",
    "        peopleBeforeNames.append(tweet['user']['screen_name'])\n",
    "print(len(peopleBeforeText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is for 2020-04-01 to 2020-04-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleAfterText = []\n",
    "peopleAfterNames = []\n",
    "with open(\"data/twitterAfter.json\", 'r') as f:\n",
    "    tweets = json.loads(f.readline())\n",
    "    for tweet in tweets:\n",
    "        peopleAfterText.append(tweet['text'])\n",
    "        peopleAfterNames.append(tweet['user']['screen_name'])\n",
    "print(len(peopleAfterText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beforeFeatures = vectorizerBefore.transform(peopleBeforeText)\n",
    "X_train, y_train = featuresBefore, labelsBefore\n",
    "\n",
    "# decision tree\n",
    "dTree = DecisionTreeClassifier()\n",
    "start_time = time.time()\n",
    "dTree = dTree.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('Decision Tree', time.time() - start_time))\n",
    "\n",
    "accuracy = dTree.score(beforeFeatures, peopleBeforeNames)\n",
    "print(\"decision tree accuracy: %.3f\" %accuracy)\n",
    "\n",
    "# knn\n",
    "k = 20\n",
    "predictor = neighbors.KNeighborsClassifier(k)\n",
    "start_time = time.time()\n",
    "predictor.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('KNN', time.time() - start_time))\n",
    "\n",
    "accuracy = predictor.score(beforeFeatures, peopleBeforeNames)\n",
    "print(\"KNN accuracy: %.3f\" %accuracy)\n",
    "\n",
    "# random forest\n",
    "predictor = RandomForestClassifier()\n",
    "start_time = time.time()\n",
    "predictor.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('random forest', time.time() - start_time))\n",
    "\n",
    "predictor.predict(beforeFeatures)\n",
    "accuracy = predictor.score(beforeFeatures, peopleBeforeNames)\n",
    "print(\"random forest: %.3f\" % accuracy)\n",
    "\n",
    "# logistic regression\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('LogisticRegression', time.time() - start_time))\n",
    "y_pred = clf.predict(beforeFeatures)\n",
    "print(\"log reg: %.3f\" % accuracy_score(peopleBeforeNames, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared to the more recent tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afterFeatures = vectorizerAfter.transform(peopleAfterText)\n",
    "X_train, X_test, y_train, y_test = train_test_split(featuresAfter, labelsAfter, test_size=0.33, random_state=42) \n",
    "\n",
    "# decision tree\n",
    "dTree = DecisionTreeClassifier()\n",
    "start_time = time.time()\n",
    "dTree = dTree.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('Decision Tree', time.time() - start_time))\n",
    "\n",
    "accuracy = dTree.score(afterFeatures, peopleAfterNames)\n",
    "print(\"decision tree accuracy: %.3f\" %accuracy)\n",
    "\n",
    "# knn\n",
    "k = 20\n",
    "predictor = neighbors.KNeighborsClassifier(k)\n",
    "start_time = time.time()\n",
    "predictor.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('KNN', time.time() - start_time))\n",
    "\n",
    "accuracy = predictor.score(afterFeatures, peopleAfterNames)\n",
    "print(\"KNN accuracy: %.3f\" %accuracy)\n",
    "\n",
    "# random forest\n",
    "predictor = RandomForestClassifier()\n",
    "start_time = time.time()\n",
    "predictor.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('random forest', time.time() - start_time))\n",
    "\n",
    "predictor.predict(afterFeatures)\n",
    "accuracy = predictor.score(afterFeatures, peopleAfterNames)\n",
    "print(\"random forest: %.3f\" % accuracy)\n",
    "\n",
    "# logistic regression\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print('Time for %s fitting: %.3f' % ('LogisticRegression', time.time() - start_time))\n",
    "y_pred = clf.predict(afterFeatures)\n",
    "print(\"log reg: %.3f\" % accuracy_score(peopleAfterNames, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}